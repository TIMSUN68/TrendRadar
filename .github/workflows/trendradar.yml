name: TrendRadar Crawler & Notify
# 工作流名称，可自定义

# 触发条件：1. 定时执行（每天 UTC 1点、10点 = 北京时间 9点、18点）；2. 手动触发
on:
  schedule:
    - cron: '0 1,10 * * *'
  workflow_dispatch:

# 执行的任务
jobs:
  run-trendradar:
    runs-on: ubuntu-latest
    steps:
      # 步骤1：拉取仓库代码
      - name: Checkout code
        uses: actions/checkout@v4

      # 步骤2：安装 Python 环境（3.9 适配多数 TrendRadar 分支）
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      # 步骤3：安装依赖（补充爬虫必需的系统依赖 + Python 依赖）
      - name: Install dependencies
        run: |
          # 安装系统依赖（部分爬虫需要浏览器环境、编译依赖）
          sudo apt-get update && sudo apt-get install -y chromium-browser gcc musl-dev
          # 升级 pip + 安装核心依赖
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # 补充邮件 + 爬虫依赖（避免缺失）
          pip install secure-smtplib python-dotenv requests_html chromium-brower

      # 步骤4：执行爬虫 + 邮件推送（核心修复：命令格式+数据验证）
      - name: Run crawler and send email notify
        env:
          # 从 Secrets 读取 QQ 邮件配置（保持不变）
          EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_TO: ${{ secrets.EMAIL_TO }}
          EMAIL_SMTP_SERVER: ${{ secrets.EMAIL_SMTP_SERVER }}
          EMAIL_SMTP_PORT: ${{ secrets.EMAIL_SMTP_PORT }}
        run: |
          # 关键修复1：正确的多命令格式（用 | 包裹，每行一个命令）
          # 关键修复2：按仓库实际入口文件选择（先试 run.py，不行换 main.py/trendradar.py）
          if [ -f "run.py" ]; then
            echo "执行入口文件：run.py"
            python run.py
          elif [ -f "main.py" ]; then
            echo "执行入口文件：main.py"
            python main.py
          elif [ -f "trendradar.py" ]; then
            echo "执行入口文件：trendradar.py"
            python trendradar.py
          else
            echo "错误：未找到入口文件！请检查仓库根目录是否有 run.py/main.py/trendradar.py"
            exit 1  # 终止工作流，避免无意义推送
          fi

          # 关键修复3：验证爬取数据是否存在（排错核心步骤）
          echo -e "\n=== 验证爬取数据 ==="
          # 常见数据存储目录：output/、data/、instance/，根据仓库实际情况修改
          DATA_DIRS=("output" "data" "instance" "crawler/output")
          FOUND_DATA=false
          for dir in "${DATA_DIRS[@]}"; do
            if [ -d "$dir" ]; then
              echo "找到数据目录：$dir"
              # 查看目录下的文件
              ls -l "$dir"
              # 查看文件内容（取第一个非空文件）
              DATA_FILE=$(find "$dir" -type f -not -empty | head -n 1)
              if [ -n "$DATA_FILE" ]; then
                echo -e "\n数据文件内容（前10行）："
                head -n 10 "$DATA_FILE"
                FOUND_DATA=true
                break
              fi
            fi
          done

          # 若未找到数据，终止工作流（避免发送空邮件）
          if [ "$FOUND_DATA" = false ]; then
            echo -e "\n错误：未爬取到任何数据！"
            exit 1
          fi
